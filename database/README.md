# Vocabulo Project Database

## Overview

This directory contains all the necessary components for setting up and managing the Vocabulo project database. 
The database is designed to support both Vocabulo Quiz and Vocabulo Junior applications, providing a comprehensive 
structure for vocabulary learning and management.

## Structure

```
database/
├── schema/           # SQL scripts for database structure and initial data
├── docker/           # Docker configurations for database initialization
│   ├── full_init/    # Full initialization from scratch
│   └── quick_init/   # Quick initialization using a pre-made dump
├── scripts/          # Additional scripts (e.g., scraping, data processing)
├── README.md         # This file
└── DB_documentation.md  # Detailed database schema documentation
```

## Data Sources

### Elix Dictionary Scraping

The primary source of our vocabulary data is the Elix LSF (French Sign Language) dictionary. 
We've developed a set of Python scripts to ethically scrape this data, respecting the website's
`robots.txt` specifications and usage terms. Here's an overview of our scraping process:

#### Scraping Scripts

1. **urlword.py**
   - Purpose: Collects URLs for all word entries in the Elix LSF dictionary.
   - Process:
     - Parses the main sitemap to find letter-specific sitemaps.
     - Extracts word URLs from each letter's sitemap.
   - Output: A text file (`elix_lsf_urls.txt`) containing URLs for all dictionary entries.

2. **scrap_Elix_words.py**
   - Purpose: Extracts all words from the Elix LSF dictionary.
   - Process:
     - Iterates through each letter's pages in the dictionary.
     - Collects words from each page.
   - Output: A text file (`mots.txt`) containing all words found in the dictionary.

3. **automate_scrapWord.py**
   - Purpose: Automates the process of scraping detailed information for each word.
   - Process:
     - Reads URLs from the file generated by `urlword.py`.
     - Extracts detailed information (definition, grammatical category, video URLs) for each word.
   - Output: A CSV file (`elix_lsf_data.csv`) containing comprehensive data for each word.

4. **ScraptOneWord.py**
   - Purpose: Allows scraping of information for a single, specific word.
   - Process:
     - Takes a word as input from the user.
     - Fetches and extracts detailed information for that word.
   - Output: Appends the word's information to a CSV file (`elix_lsf_data_single.csv`).

#### Scraping Process

1. We first run `urlword.py` to gather all the URLs for the dictionary entries.
2. Then, `scrap_Elix_words.py` is used to create a comprehensive list of all words in the dictionary.
3. `automate_scrapWord.py` is the main script used to collect detailed information for all words, generating our primary dataset.
4. `ScraptOneWord.py` is used as needed to fetch information for specific words that might be missing or need updating.

#### Ethical Considerations

- We've implemented appropriate delays between requests to avoid overwhelming the Elix server.
- Our scraping respects the `robots.txt` file of the Elix website.
- We've reached out to Elix to ensure our use of the data is permissible and aligns with their terms of service.

#### Database Integration

The processed data from these scraping scripts is then imported into our database using the `03_import_data_csv.sql` script located in the `schema/` directory. This script:

- Reads the CSV files generated by our scraping scripts.
- Inserts the data into the appropriate tables in our database structure.
- Handles any necessary data transformations or relationships during the import process.

#### Maintenance and Updates

To keep our database current with the Elix LSF dictionary:

- In the future, we periodically re-run our scraping scripts to check for new or updated entries.
- The `ScraptOneWord.py` script is particularly useful for quick updates or additions of specific words.

For more details on the specifics of each script, please refer to the inline documentation within each Python file in the `scripts/` directory.

### Thematic Word Lists

We've supplemented the Elix data with thematic word lists derived from:
- Educational resources for primary school vocabulary
- French as a Foreign Language (FLE) resources
- Custom-curated lists for specific learning domains

These lists are used to create categories and subcategories, enhancing the learning experience by providing contextual 
groupings of words.

## Word Difficulty Scoring

A key feature of our database is the calculation of a difficulty score for each word. This score is derived from 
multiple sources and factors to provide a comprehensive assessment of a word's complexity.

### Data Sources for Difficulty Calculation

1. **Word Frequency in Books and Films**
   - We've incorporated frequency data from a large corpus of French books and films.
   - Words that appear more frequently are generally considered easier to learn and recognize.

2. **Manulex Data**
   - We use the Manulex database, which provides word frequency data from French school textbooks.
   - This gives us insight into which words students are likely to encounter in their academic materials.

3. **Dubois-Buyse Scale**
   - The Dubois-Buyse scale is a classic French orthographic scale that ranks words based on the age at which children 
   typically master their spelling.
   - It provides 42 echelons, corresponding roughly to grade levels from CP (1st grade) to the end of collège (9th grade).

4. **Syllable Count**
   - The number of syllables in a word is factored into the difficulty calculation, as longer words are often more 
   challenging.

### Calculation Process

Our difficulty score is calculated using a weighted formula that takes into account:

1. **Frequency Scores**: 
   - Words with higher frequency in books, films, and textbooks receive lower difficulty scores.

2. **Dubois-Buyse Echelon**: 
   - Words in higher echelons (learned later in a child's education) receive higher difficulty scores.

3. **Syllable Count**: 
   - Words with more syllables generally receive higher difficulty scores.

4. **Grammatical Category**: 
   - Different weights are assigned based on the word's grammatical category (e.g., nouns, verbs, adjectives).

The final score is normalized to a scale of 1-100, where:
- 1-33: Easy
- 34-66: Medium
- 67-100: Hard

### Implementation

The difficulty score calculation is implemented in the `06_score_diff.sql` script. This script:

1. Normalizes the various input factors.
2. Applies the weighting formula.
3. Stores the calculated difficulty score in the `mot` table.

### Usage in the Application

This difficulty score is used by the recommendation system to:
- Tailor word suggestions to a user's current skill level.
- Provide a balanced mix of easy, medium, and hard words in quizzes.
- Track user progress by monitoring performance across different difficulty levels.

### Future Improvements

We are considering enhancing our difficulty scoring by:
- Incorporating user feedback on perceived word difficulty.
- Adjusting scores based on actual user performance data.
- Integrating more nuanced linguistic features, such as homonyms or idiomatic usage.

For more technical details on the scoring algorithm, please refer to the `06_score_diff.sql` file in the `schema/` directory.

## Database Initialization

Due to the proprietary nature of the data and its volume, the actual data files are not included in this repository. 
However, we provide two methods for initializing the database structure:

### Full Initialization

Located in `docker/full_init/`, this method sets up the complete database structure from scratch. It's ideal for 
development and testing environments where you need to understand or modify the base structure.

To use:
1. Navigate to `docker/full_init/`
2. Copy `.env.example` to `.env` and adjust values
3. Run `docker-compose up -d`

Note: Without the actual data files, this will only create the structure without populating it.

### Quick Initialization

Located in `docker/quick_init/`, this method uses a pre-made database dump for faster setup. It's suitable for staging 
or demonstration environments where you need a functional database quickly.

To use:
1. Navigate to `docker/quick_init/`
2. Copy `.env.example` to `.env` and adjust values
3. Place the `lsf_app_dump.sql` file in the same directory (not provided in the repository)
4. Run `docker-compose up -d`
5. Execute the provided script to restore the dump and reset passwords

## Database Management

We've included pgAdmin in our Docker setup to facilitate database management for team members. This provides a 
user-friendly interface for exploring and managing the database structure and content.

To access pgAdmin:
1. Ensure the Docker containers are running
2. Navigate to `http://localhost:5050` in your web browser
3. Log in using the credentials specified in your `.env` file


## Security and Collaboration

- Different user roles (read-only and read-write) are set up for team collaboration. Refer to `schema/08_create_user.sql` 
for details.
- Ensure all sensitive information (passwords, API keys) are stored in the `.env` file and never committed to the 
repository.

## Documentation

For a detailed explanation of the database schema, relationships, and design decisions, refer to `DB_documentation.md`.

## Future Improvements

- Implement automated data updates from Elix (pending approval)
- Develop a more robust migration system for database schema changes

