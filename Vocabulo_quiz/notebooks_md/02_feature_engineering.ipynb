{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02 - Feature Engineering and Selection\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook focuses on feature engineering and selection for the VocabuloQuizz recommendation system. Based on the insights from my exploratory data analysis, I will create new features, transform existing ones, and select the most relevant features for my model.\n",
    "This notebook presents the evolution of my approach to data preparation and modelling, showing how I have iteratively improved my system to meet the specific challenges of this project.\n",
    "\n",
    "It's important to note that we are working with synthetically generated data, which allows us to experiment freely but may not fully represent real-world user behaviors.\n",
    "\n",
    "## 1. Data Loading and Preparation"
   ],
   "id": "94c1fc8e8b0ff01d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### initial dataprocessing :\n",
    "Script in src/feature_engineering/data_prep_initial.py"
   ],
   "id": "d860ab0197aa8ea2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T11:32:30.827854Z",
     "start_time": "2024-09-11T11:32:30.726757Z"
    }
   },
   "cell_type": "code",
   "source": "from Vocabulo_quiz.src.feature_engineering.data_prep_initial import prepare_data_for_model",
   "id": "75c493c719efe078",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T11:40:38.789788Z",
     "start_time": "2024-09-11T11:40:37.767597Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X, y, user_features, word_features = prepare_data_for_model()\n",
    "print(\"User features:\")\n",
    "print(user_features.head())\n",
    "\n",
    "print(\"Word features:\")\n",
    "print(word_features.head())"
   ],
   "id": "5588ec4ae287b7fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://lsf_user:lsf_password@localhost:5432/lsf_app\n",
      "Successful connection to the database.\n",
      "Data loaded successfully.\n",
      "Quiz data preview:\n",
      "                                quiz_id                      date mot_id  \\\n",
      "0  555e9ba5-3764-4728-acf6-d7f0e32d8aa8 2024-06-07 00:21:36+00:00   None   \n",
      "1  be0972b2-c5f8-4af1-907a-f1cd6968f798 2024-06-08 00:21:36+00:00   None   \n",
      "2  d6f5761d-1357-4e15-8d14-826935baaa46 2024-06-09 00:21:36+00:00   None   \n",
      "3  33ade695-7276-4676-918f-799f5179b37c 2024-06-10 00:21:36+00:00   None   \n",
      "4  a7201a26-a7b9-477f-90e2-0d52875e4e6a 2024-06-11 00:21:36+00:00   None   \n",
      "\n",
      "                                user_id  \n",
      "0  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "1  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "2  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "3  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "4  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "\n",
      "Score data preview:\n",
      "                               score_id                               quiz_id  \\\n",
      "0  7700fd23-d106-4082-9d70-20b8b73937f2  555e9ba5-3764-4728-acf6-d7f0e32d8aa8   \n",
      "1  1be1a7e9-0a4b-4da3-9324-be4fcfdeff79  555e9ba5-3764-4728-acf6-d7f0e32d8aa8   \n",
      "2  f0c44a83-6815-4863-8bff-9df9fcd04a36  be0972b2-c5f8-4af1-907a-f1cd6968f798   \n",
      "3  c65c02e0-4240-4769-a44d-ffb127ede74a  be0972b2-c5f8-4af1-907a-f1cd6968f798   \n",
      "4  d3e7eed7-548d-4fe2-911c-8c55290b4454  d6f5761d-1357-4e15-8d14-826935baaa46   \n",
      "\n",
      "   mot_id  score  use_sign  count  \n",
      "0   24824   True      True      2  \n",
      "1   33086  False     False      1  \n",
      "2   28854   True      True      1  \n",
      "3   71948  False      True      0  \n",
      "4    1565   True      True      0  \n",
      "Available columns in pairs: Index(['quiz_id', 'date', 'mot_id_x', 'user_id', 'score_id', 'mot_id_y',\n",
      "       'score', 'use_sign', 'count'],\n",
      "      dtype='object')\n",
      "User features:\n",
      "                                user_id            pseudo    password  \\\n",
      "0  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  martineaumonique  ju1!FT!M^1   \n",
      "1  ff96ac45-db30-4055-b12e-1b52555c66e8    diasmarguerite  p9Qy$y6C*a   \n",
      "2  d91cd45b-af08-46be-a8f6-4104109a78c3     marievalentin  )e6FbrhLcs   \n",
      "3  2cc5f0ce-ab96-49dc-a14a-d4406e14a4cd           plefort  un5GsGgtN@   \n",
      "4  f5b12321-07c6-4c29-948f-2659946cf366      juleslegrand  WKTek1Lg^0   \n",
      "\n",
      "                       date  quiz_id  token_id  quiz_count  avg_score  \\\n",
      "0 2024-03-30 17:46:57+00:00        0         1    2.727636   2.965382   \n",
      "1 2024-01-29 23:58:59+00:00        0         2    2.727636   2.574255   \n",
      "2 2024-02-12 19:33:59+00:00        0         3    2.727636   2.236282   \n",
      "3 2024-06-21 23:53:58+00:00        0         4    2.727636   3.593564   \n",
      "4 2024-05-10 07:42:38+00:00        0         5    2.727636   2.259374   \n",
      "\n",
      "       Bien  Trop dur  Trop facile  \n",
      "0  3.107357  2.690702     2.298718  \n",
      "1  2.442800  2.850377     2.799322  \n",
      "2  2.855796  2.134175     3.095494  \n",
      "3  2.670948  2.592680     2.827152  \n",
      "4  2.131106  2.928736     3.030706  \n",
      "Word features:\n",
      "   mot_id     mot                                      definition  \\\n",
      "0   73453    sept  chiffre, nombre compris entre six et huit (7).   \n",
      "1   68221    rond                     qui a une forme circulaire.   \n",
      "2   45958    lent      qui est sans énergie, mou, sans caractère.   \n",
      "3   48554  manger                    absorber (de la nourriture).   \n",
      "4    8094   barbe                       cheval d'Afrique du Nord.   \n",
      "\n",
      "   alphabet_id  frequence  niv_diff_id  echelon_id  avg_difficulty  frequency  \\\n",
      "0         19.0         33            1        19.0       -0.354865  -0.492003   \n",
      "1         18.0         33            1        19.0       -0.354865  -0.492003   \n",
      "2         12.0         33            1        19.0       -0.354865   3.548420   \n",
      "3         13.0         35            1        14.0       -0.354865  -0.492003   \n",
      "4          2.0         61            3         0.0       -0.354865   1.528209   \n",
      "\n",
      "   gram_1.0  gram_2.0  gram_3.0  gram_4.0  gram_5.0  gram_6.0  gram_7.0  \\\n",
      "0         0         0         1         0         0         0         0   \n",
      "1         0         0         1         0         0         0         0   \n",
      "2         0         0         1         0         0         0         0   \n",
      "3         0         1         0         0         0         0         0   \n",
      "4         1         0         0         0         0         0         0   \n",
      "\n",
      "   gram_8.0  gram_9.0  gram_10.0  \n",
      "0         0         0          0  \n",
      "1         0         0          0  \n",
      "2         0         0          0  \n",
      "3         0         0          0  \n",
      "4         0         0          0  \n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this stage:\n",
    "* **User features** were calculated based on quiz participation and average scores\n",
    "* **Word features** were based on frequency and difficulty, with some basic normalization\n",
    "* **Normalization** was done using `StandardScaler` for numeric features, a simple but effective approach at this point."
   ],
   "id": "6ca538167e2863b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Observations**:\n",
    "- The initial data preparation provides basic user and word features.\n",
    "- User features include quiz count, average score, and evaluation distributions.\n",
    "- Word features contain basic information like difficulty, score_diff(frequence), and grammatical categories.\n",
    "\n",
    "**Limitations**:\n",
    "- Lack of temporal features: No consideration of time-based patterns in user behavior.\n",
    "- Limited word representation: Simple categorical encoding for words, missing semantic information.\n",
    "- Absence of interaction features: No features capturing the relationship between users and words.\n",
    "- Basic normalization: Simple standardization of numeric features may not capture complex patterns.\n"
   ],
   "id": "534bb54162cf603e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### initial data processing ameliorated\n",
    "File in ../src/feature_engineering/data_prep_ameliored.py"
   ],
   "id": "a0cb9bd0d336acfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As the project evolved, I expanded the feature set and improved the data preprocessing techniques. New pipelines were introduced to handle categorical and missing data more efficiently.\n",
    "\n",
    "This script introduce :\n",
    "- **Missing value handling** with `SimpleImputer`\n",
    "- **Categorical data encoding** using `OneHotEncoder`\n",
    "- **Pipeline** to manage the preprocessing of both numerical and categorical features efficiently"
   ],
   "id": "d32cf731bbc04fbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:55:10.100563Z",
     "start_time": "2024-09-11T08:55:09.350151Z"
    }
   },
   "cell_type": "code",
   "source": "from Vocabulo_quiz.src.feature_engineering.data_prep_ameliored import prepare_data_for_model",
   "id": "5317a3a08b90cc43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://lsf_user:lsf_password@localhost:5432/lsf_app\n",
      "Successful connection to the database.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T08:55:11.274276Z",
     "start_time": "2024-09-11T08:55:10.175926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "X, y, preprocessor = prepare_data_for_model()\n",
    "print(\"Data preparation complete.\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Features: {X.columns.tolist()}\")\n",
    "print(f\"Number of features after preprocessing: {X.shape[1]}\")\n",
    "print(f\"Example rows from X:\")\n",
    "print(X.head())\n",
    "print(f\"Distribution of y: {np.bincount(y)}\")"
   ],
   "id": "ea7e75377a8dbb6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://lsf_user:lsf_password@localhost:5432/lsf_app\n",
      "Successful connection to the database.\n",
      "Data loaded successfully.\n",
      "Quiz data preview:\n",
      "                                quiz_id                      date mot_id  \\\n",
      "0  555e9ba5-3764-4728-acf6-d7f0e32d8aa8 2024-06-07 00:21:36+00:00   None   \n",
      "1  be0972b2-c5f8-4af1-907a-f1cd6968f798 2024-06-08 00:21:36+00:00   None   \n",
      "2  d6f5761d-1357-4e15-8d14-826935baaa46 2024-06-09 00:21:36+00:00   None   \n",
      "3  33ade695-7276-4676-918f-799f5179b37c 2024-06-10 00:21:36+00:00   None   \n",
      "4  a7201a26-a7b9-477f-90e2-0d52875e4e6a 2024-06-11 00:21:36+00:00   None   \n",
      "\n",
      "                                user_id  \n",
      "0  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "1  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "2  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "3  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "4  1e34bff7-8f44-496b-aed5-ecb19ea96eb0  \n",
      "\n",
      "Score data preview:\n",
      "                               score_id                               quiz_id  \\\n",
      "0  7700fd23-d106-4082-9d70-20b8b73937f2  555e9ba5-3764-4728-acf6-d7f0e32d8aa8   \n",
      "1  1be1a7e9-0a4b-4da3-9324-be4fcfdeff79  555e9ba5-3764-4728-acf6-d7f0e32d8aa8   \n",
      "2  f0c44a83-6815-4863-8bff-9df9fcd04a36  be0972b2-c5f8-4af1-907a-f1cd6968f798   \n",
      "3  c65c02e0-4240-4769-a44d-ffb127ede74a  be0972b2-c5f8-4af1-907a-f1cd6968f798   \n",
      "4  d3e7eed7-548d-4fe2-911c-8c55290b4454  d6f5761d-1357-4e15-8d14-826935baaa46   \n",
      "\n",
      "   mot_id  score  use_sign  count  \n",
      "0   24824   True      True      2  \n",
      "1   33086  False     False      1  \n",
      "2   28854   True      True      1  \n",
      "3   71948  False      True      0  \n",
      "4    1565   True      True      0  \n",
      "Colonnes disponibles dans pairs: Index(['quiz_id', 'date', 'mot_id_x', 'user_id', 'score_id', 'mot_id_y',\n",
      "       'score', 'use_sign', 'count'],\n",
      "      dtype='object')\n",
      "Types of columns in final_data:\n",
      "user_id                        object\n",
      "mot_id                          int64\n",
      "score                            bool\n",
      "pseudo                         object\n",
      "password                       object\n",
      "date              datetime64[ns, UTC]\n",
      "quiz_id                         int64\n",
      "token_id                        int64\n",
      "quiz_count                    float64\n",
      "avg_score                     float64\n",
      "Bien                          float64\n",
      "Trop dur                      float64\n",
      "Trop facile                   float64\n",
      "mot                            object\n",
      "definition                     object\n",
      "alphabet_id                   float64\n",
      "frequence                       int64\n",
      "niv_diff_id                     int64\n",
      "echelon_id                    float64\n",
      "avg_difficulty                float64\n",
      "frequency                     float64\n",
      "gram_1.0                        uint8\n",
      "gram_2.0                        uint8\n",
      "gram_3.0                        uint8\n",
      "gram_4.0                        uint8\n",
      "gram_5.0                        uint8\n",
      "gram_6.0                        uint8\n",
      "gram_7.0                        uint8\n",
      "gram_8.0                        uint8\n",
      "gram_9.0                        uint8\n",
      "gram_10.0                       uint8\n",
      "dtype: object\n",
      "\n",
      "Unique values in categorical columns:\n",
      "niv_diff_id: [3 1 2]\n",
      "echelon_id: [ 0. 15. 14. 21. 18. 22. 12. 24. 31. 16. 17.  9.  8. 36. 19. 20. 25. 23.\n",
      " 29. 11. 33. 13. 30. 10.  1. 28. 27. 34. 26.  7.  5. 42. 38. 35. 32. 39.\n",
      "  6.]\n",
      "gram_1.0: [1 0]\n",
      "gram_2.0: [0 1]\n",
      "gram_3.0: [0 1]\n",
      "gram_4.0: [0 1]\n",
      "gram_5.0: [0 1]\n",
      "gram_6.0: [0 1]\n",
      "gram_7.0: [0 1]\n",
      "gram_8.0: [0 1]\n",
      "gram_9.0: [0 1]\n",
      "gram_10.0: [0 1]\n",
      "Colonnes disponibles dans final_data: ['user_id', 'mot_id', 'score', 'pseudo', 'password', 'date', 'quiz_id', 'token_id', 'quiz_count', 'avg_score', 'Bien', 'Trop dur', 'Trop facile', 'mot', 'definition', 'alphabet_id', 'frequence', 'niv_diff_id', 'echelon_id', 'avg_difficulty', 'frequency', 'gram_1.0', 'gram_2.0', 'gram_3.0', 'gram_4.0', 'gram_5.0', 'gram_6.0', 'gram_7.0', 'gram_8.0', 'gram_9.0', 'gram_10.0']\n",
      "Features numériques utilisées: ['quiz_count', 'avg_score', 'Bien', 'Trop dur', 'Trop facile', 'frequence', 'avg_difficulty', 'frequency', 'alphabet_id']\n",
      "Features catégorielles utilisées: ['niv_diff_id', 'echelon_id', 'gram_1.0', 'gram_2.0', 'gram_3.0', 'gram_4.0', 'gram_5.0', 'gram_6.0', 'gram_7.0', 'gram_8.0', 'gram_9.0', 'gram_10.0']\n",
      "Data preparation complete.\n",
      "X shape: (20839, 71)\n",
      "y shape: (20839,)\n",
      "Features: ['quiz_count', 'avg_score', 'Bien', 'Trop dur', 'Trop facile', 'frequence', 'avg_difficulty', 'frequency', 'alphabet_id', 'niv_diff_id_1.0', 'niv_diff_id_2.0', 'niv_diff_id_3.0', 'echelon_id_0.0', 'echelon_id_1.0', 'echelon_id_5.0', 'echelon_id_6.0', 'echelon_id_7.0', 'echelon_id_8.0', 'echelon_id_9.0', 'echelon_id_10.0', 'echelon_id_11.0', 'echelon_id_12.0', 'echelon_id_13.0', 'echelon_id_14.0', 'echelon_id_15.0', 'echelon_id_16.0', 'echelon_id_17.0', 'echelon_id_18.0', 'echelon_id_19.0', 'echelon_id_20.0', 'echelon_id_21.0', 'echelon_id_22.0', 'echelon_id_23.0', 'echelon_id_24.0', 'echelon_id_25.0', 'echelon_id_26.0', 'echelon_id_27.0', 'echelon_id_28.0', 'echelon_id_29.0', 'echelon_id_30.0', 'echelon_id_31.0', 'echelon_id_32.0', 'echelon_id_33.0', 'echelon_id_34.0', 'echelon_id_35.0', 'echelon_id_36.0', 'echelon_id_38.0', 'echelon_id_39.0', 'echelon_id_42.0', 'gram_1.0_0.0', 'gram_1.0_1.0', 'gram_2.0_0.0', 'gram_2.0_1.0', 'gram_3.0_0.0', 'gram_3.0_1.0', 'gram_4.0_0.0', 'gram_4.0_1.0', 'gram_5.0_0.0', 'gram_5.0_1.0', 'gram_6.0_0.0', 'gram_6.0_1.0', 'gram_7.0_0.0', 'gram_7.0_1.0', 'gram_8.0_0.0', 'gram_8.0_1.0', 'gram_9.0_0.0', 'gram_9.0_1.0', 'gram_10.0_0.0', 'gram_10.0_1.0', 'user_id', 'mot_id']\n",
      "Number of features after preprocessing: 71\n",
      "Example rows from X:\n",
      "     quiz_count  avg_score      Bien  Trop dur  Trop facile  frequence  \\\n",
      "0 -4.440892e-16   0.838249  0.937920 -0.017310    -0.970826  -1.317080   \n",
      "1 -4.440892e-16  -1.621944 -1.072652  0.874894     0.205467  -1.317080   \n",
      "2 -4.440892e-16   0.838249  0.937920 -0.017310    -0.970826   0.083896   \n",
      "3 -4.440892e-16   0.838249  0.937920 -0.017310    -0.970826   0.730500   \n",
      "4 -4.440892e-16   0.838249  0.937920 -0.017310    -0.970826   0.407198   \n",
      "\n",
      "   avg_difficulty  frequency  alphabet_id  niv_diff_id_1.0  ...  gram_7.0_0.0  \\\n",
      "0       -0.006376   1.493638    -0.895844              0.0  ...           1.0   \n",
      "1       -0.006376   1.493638    -0.895844              0.0  ...           1.0   \n",
      "2        1.056629  -0.496892    -0.607300              0.0  ...           1.0   \n",
      "3       -1.069382  -0.496892    -0.751572              0.0  ...           1.0   \n",
      "4        1.056629   1.493638     1.268234              0.0  ...           1.0   \n",
      "\n",
      "   gram_7.0_1.0  gram_8.0_0.0  gram_8.0_1.0  gram_9.0_0.0  gram_9.0_1.0  \\\n",
      "0           0.0           1.0           0.0           1.0           0.0   \n",
      "1           0.0           1.0           0.0           1.0           0.0   \n",
      "2           0.0           1.0           0.0           1.0           0.0   \n",
      "3           0.0           0.0           1.0           1.0           0.0   \n",
      "4           0.0           1.0           0.0           1.0           0.0   \n",
      "\n",
      "   gram_10.0_0.0  gram_10.0_1.0                               user_id  mot_id  \n",
      "0            1.0            0.0  1e34bff7-8f44-496b-aed5-ecb19ea96eb0   24824  \n",
      "1            1.0            0.0  057bcffc-ac5a-4a02-9270-e1813a05df95   24824  \n",
      "2            1.0            0.0  1e34bff7-8f44-496b-aed5-ecb19ea96eb0   33086  \n",
      "3            1.0            0.0  1e34bff7-8f44-496b-aed5-ecb19ea96eb0   28854  \n",
      "4            1.0            0.0  1e34bff7-8f44-496b-aed5-ecb19ea96eb0   71948  \n",
      "\n",
      "[5 rows x 71 columns]\n",
      "Distribution of y: [10482 10357]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Improvements in v2**:\n",
    "- Enhanced user profiling: More detailed user-level features capturing learning patterns.\n",
    "- Improved word representation: Additional word-level features providing more context.\n",
    "- Basic temporal features: Introduction of time-based features to capture user progress over time.\n",
    "\n",
    "**Remaining challenges**:\n",
    "- Still lacking advanced NLP techniques for word representation.\n",
    "- Limited consideration of user-word interactions.\n",
    "- Room for improvement in handling categorical variables."
   ],
   "id": "e3e2a08dff8bf11e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Final data processing\n",
    "File in ../src/feature_engineering/data_prep_final.py\n",
    "\n",
    "In the final phase, the preprocessing pipeline was expanded further to include more complex features and time-based transformations.\n"
   ],
   "id": "54c2d7a18071470"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:03:09.103592Z",
     "start_time": "2024-09-11T13:03:09.017264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from Vocabulo_quiz.src.feature_engineering.data_prep_final import prepare_data_for_model\n",
    "\n",
    "X, y, preprocessor = prepare_data_for_model()\n",
    "print(\"Data preparation complete.\")\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ],
   "id": "2aca920254cdcdb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://lsf_user:lsf_password@localhost:5432/lsf_app\n",
      "Successful connection to the database.\n",
      "Data loaded. DataFrame shape: (576, 18)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Engine' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mVocabulo_quiz\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_engineering\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_prep_final\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m prepare_data_for_model\n\u001B[1;32m----> 3\u001B[0m X, y, preprocessor \u001B[38;5;241m=\u001B[39m prepare_data_for_model()\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData preparation complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShape of X: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mX\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Vocabulo_ML\\Vocabulo_quiz\\src\\feature_engineering\\data_prep_final.py:114\u001B[0m, in \u001B[0;36mprepare_data_for_model\u001B[1;34m()\u001B[0m\n\u001B[0;32m    112\u001B[0m df \u001B[38;5;241m=\u001B[39m prepare_training_data(conn)\n\u001B[0;32m    113\u001B[0m X, y, preprocessor \u001B[38;5;241m=\u001B[39m preprocess_data(df)\n\u001B[1;32m--> 114\u001B[0m conn\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y, preprocessor\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Engine' object has no attribute 'close'"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Key advancements in data preparation:\n",
    "\n",
    "1. Comprehensive data retrieval:\n",
    "   - Efficient SQL query to gather relevant data from multiple tables in a single operation.\n",
    "   - Inclusion of user feedback data (eval_mot) for a more holistic view of user interactions.\n",
    "\n",
    "2. Sophisticated temporal features:\n",
    "   - Conversion of timestamps to datetime objects for easier manipulation.\n",
    "   - Extraction of hour, day of week, and month from quiz dates.\n",
    "   - Cyclical encoding of time features (hour, day of week, month) to capture periodic patterns.\n",
    "   - Calculation of 'days_since_last_seen' to capture recency effects.\n",
    "\n",
    "3. Advanced feature preprocessing:\n",
    "   - Standardization of numeric features using StandardScaler for consistent scale across variables.\n",
    "   - One-hot encoding of categorical variables, including handling of unknown categories.\n",
    "   - Retention of cyclical features without further transformation to preserve their inherent structure.\n",
    "\n",
    "4. Flexible preprocessing pipeline:\n",
    "   - Use of ColumnTransformer to apply different preprocessing steps to different types of features.\n",
    "   - Easy addition or modification of feature transformations for future improvements.\n",
    "\n",
    "5. Integrated word difficulty metrics:\n",
    "   - Incorporation of various word difficulty measures (freqfilms, freqlivres, nbr_syll, cp_cm2_sfi) for a multi-faceted representation of word complexity.\n",
    "\n",
    "6. User interaction history:\n",
    "   - Inclusion of user-specific interaction data (times_correct, times_seen) to capture individual learning patterns.\n",
    "\n",
    "7. Contextual features:\n",
    "   - Integration of category and subcategory information to provide context for each word.\n",
    "   - Inclusion of grammatical information (gramm_id) for linguistic context.\n",
    "\n",
    "These improvements provide a robust foundation for my model, capturing a wide range of factors that may influence a user's performance on a given word. The preprocessing pipeline ensures that all features are appropriately scaled and encoded for machine learning algorithms.\n",
    "\n",
    "Future directions for enhancement could include:\n",
    "- Implementation of word embeddings (e.g., Word2Vec, GloVe) for richer semantic representation of words.\n",
    "- Exploration of more advanced feature interactions, particularly between user history and word characteristics (but on real data).\n",
    "- Investigation of time series features to capture trends in user performance over time.\n",
    "- Application of dimensionality reduction techniques (e.g., PCA) if the feature space becomes too large.\n",
    "- Integration of external language resources for additional linguistic features.\n",
    "- Experimentation with more advanced NLP techniques like BERT or transformers for word representation, if computational resources allow."
   ],
   "id": "bde75527411c75b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Preparation for Modeling",
   "id": "96cd88a5f342210b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-11T13:09:58.036316Z",
     "start_time": "2024-09-11T13:09:57.884331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data split into training and testing sets.\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ],
   "id": "acadb161568dac5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into training and testing sets.\n",
      "Training set shape: (16671, 30)\n",
      "Testing set shape: (4168, 30)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7. Conclusion and Next Steps\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "In this notebook, I have performed extensive feature engineering and selection for the VocabuloQuizz recommendation system. Key steps included:\n",
    "\n",
    "1. Creating time-based features to capture temporal patterns in user behavior.\n",
    "2. Developing user proficiency features to track learning progress.\n",
    "3. Engineering word complexity features to better represent difficulty levels.\n",
    "4. Generating interaction features to capture user-word relationships.\n",
    "5. Transforming features using standardization and one-hot encoding.\n",
    "6. Performing correlation analysis to remove redundant features.\n",
    "\n",
    "The final feature set includes a combination of engineered features and original features selected based on their importance. This refined feature set should provide a strong foundation for building our recommendation model.\n",
    "\n",
    "Next Steps:\n",
    "1. Develop and train multiple models (e.g., Random Forest, XGBoost, Neural Networks) using the selected feature set.\n",
    "2. Perform hyperparameter tuning for each model type.\n",
    "3. Evaluate and compare model performances using appropriate metrics (e.g., accuracy, F1-score, NDCG).\n",
    "4. Analyze model predictions to gain insights into the factors influencing word recommendations.\n",
    "5. Prepare for model deployment, including setting up a pipeline for real-time feature engineering.\n",
    "\n",
    "Remember that while these features and insights are based on synthetic data, they provide a solid starting point. As I transition to real user data, I should be prepared to refine our feature engineering process and model selection based on actual user behaviors and performance patterns.\n"
   ],
   "id": "f29804c16b35aea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1600c1c2867a3247"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
